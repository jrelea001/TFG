{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APRENDIZAJE SUPERVISADO\n",
    "\n",
    "En este notebook se podrá visualizar el codigo relacionado con el aprendizaje supervisado unicamente. Para visualizar el codigo relacionado con el aprendizaje no supervisado acceder al archivo de este mismo repositorio, no_supervisado.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de las librerías \n",
    "\n",
    "A continuación se importan las librerias necesarias. Adicionalmente se configuran unas variables de entorno para obtener los mismos resultados para las diferentes ejecuciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers, initializers\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Establecer semilla estática\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método personalizado para la tasa de aprendizaje que se utiliza posteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom learning rate schedule\n",
    "def custom_lr_schedule(epoch):\n",
    "    initial_lr = 1e-5\n",
    "    peak_lr = 1e-2\n",
    "    warmup_epochs = 10\n",
    "    final_epoch = 200\n",
    "    final_lr = 1e-3\n",
    "    if epoch < warmup_epochs:\n",
    "        lr = initial_lr + (peak_lr - initial_lr) * (epoch / warmup_epochs)\n",
    "    else:\n",
    "        decay_rate = np.log(final_lr / peak_lr) / (final_epoch - warmup_epochs)\n",
    "        lr = peak_lr * np.exp(decay_rate * (epoch - warmup_epochs))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_path = 'supervisado_final.csv'\n",
    "df = pd.read_csv(file_path, sep=';', decimal='.')\n",
    "\n",
    "# Data processing\n",
    "X = df.drop(['ID Usuario', 'Conclusion'], axis=1).values\n",
    "y = df['Conclusion'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapeo de etiquetas a enteros, codificación de etiquetas, escalado de datos y calculo de pesos de las calses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels to integers\n",
    "label_mapping = {0: 0, 0.5: 1, 1: 2}\n",
    "y_mapped = np.array([label_mapping[val] for val in y])\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y_mapped)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir el conjunto de datos en train (70%), test (20%) y validation (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, random_state=SEED)  # 10% of the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de la arquitectura de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_scaled.shape[1],), kernel_initializer=initializers.glorot_uniform(), kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu', kernel_initializer=initializers.glorot_uniform(), kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),    \n",
    "    Dense(64, activation='relu', kernel_initializer=initializers.glorot_uniform(), kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu', kernel_initializer=initializers.glorot_uniform(), kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-5)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3), \n",
    "    Dense(len(encoder.classes_), activation='softmax', kernel_initializer=initializers.glorot_uniform())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "lr_scheduler = LearningRateScheduler(custom_lr_schedule)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_val, y_val), class_weight=class_weights, callbacks=[lr_scheduler, model_checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar el comportamiento de la perdida y la precisión de entrenamiento y validación para las diferentes épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "plt.title('Pérdida durante el entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Precisión de entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Precisión de validación')\n",
    "plt.title('Precisión durante el entrenamiento')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisión')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener el modelo con mejores resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model('best_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar el mejor modelo obtenido con los datos de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set and print the accuracy\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy Red neuronal: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Predict and evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "# Convert the encoder.classes_ values to string once\n",
    "target_names = [str(class_name) for class_name in encoder.classes_]\n",
    "\n",
    "# Print classification report without duplicates\n",
    "print(classification_report(y_test, y_pred_classes, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar matriz de confusión para los datos en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Verdadero')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar curva ROC y AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(encoder.classes_)):\n",
    "    fpr, tpr, _ = roc_curve(y_test == i, y_pred[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'Clase {target_names[i]} (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar curva de precision-recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(encoder.classes_)):\n",
    "    precision, recall, _ = precision_recall_curve(y_test == i, y_pred[:, i])\n",
    "    plt.plot(recall, precision, label=f'Clase {target_names[i]}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precisión')\n",
    "plt.title('Curva Precision-Recall')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
