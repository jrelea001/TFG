{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AJUSTE DE HIPERPARAMETROS PARA LA RED NEURONAL PREDICTORA\n",
    "\n",
    "En este apartado se busca obtener la mejor configuración de hiperparametros dentro de un campo de busqueda.\n",
    "\n",
    "¡¡¡ MUY COSTOSO COMPUTACIONALMENTE !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias y semilla estatica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras import regularizers, initializers\n",
    "import random\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Establecer semilla estática\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasa de aprendizaje personalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom learning rate schedule\n",
    "def custom_lr_schedule(epoch):\n",
    "    initial_lr = 1e-5\n",
    "    peak_lr = 1e-2\n",
    "    warmup_epochs = 10\n",
    "    final_epoch = 200\n",
    "    final_lr = 1e-3\n",
    "    if epoch < warmup_epochs:\n",
    "        lr = initial_lr + (peak_lr - initial_lr) * (epoch / warmup_epochs)\n",
    "    else:\n",
    "        decay_rate = np.log(final_lr / peak_lr) / (final_epoch - warmup_epochs)\n",
    "        lr = peak_lr * np.exp(decay_rate * (epoch - warmup_epochs))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_path = 'supervisado_final.csv'\n",
    "df = pd.read_csv(file_path, sep=';', decimal='.')\n",
    "\n",
    "# Data processing\n",
    "X = df.drop(['ID Usuario', 'Conclusion'], axis=1).values\n",
    "y = df['Conclusion'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codificación de etiquetas, escalado de datos y calculo de pesos de las calses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir el conjuntoo de datos en train (70%), test (20%) y validation (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train, validation, and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, random_state=SEED)  # 10% of the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Campo de busqueda de configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter values to try\n",
    "layer_sizes = [[128, 64, 64, 32], [128, 64, 32, 16], [128, 32, 32, 32], [64, 32, 32, 16], [64, 32, 16, 8], [16, 8, 8, 4], [128, 64, 32], [128, 32, 32], [64, 32, 16]]\n",
    "dropout_rates = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "l1_values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "l2_values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento, obtención y visualización de la matriz de confusion para la mejor conbinación de hiperparametros explorada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for layers, dropout_rate, l1, l2 in product(layer_sizes, dropout_rates, l1_values, l2_values):\n",
    "    # Build model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layers[0], activation='relu', input_shape=(X_train.shape[1],), \n",
    "                    kernel_initializer=initializers.glorot_uniform(), kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    for layer_size in layers[1:]:\n",
    "        model.add(Dense(layer_size, activation='relu', \n",
    "                        kernel_initializer=initializers.glorot_uniform(), kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(len(encoder.classes_), activation='softmax', kernel_initializer=initializers.glorot_uniform()))\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "    lr_scheduler = LearningRateScheduler(custom_lr_schedule)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_val, y_val), \n",
    "                        class_weight=class_weights, callbacks=[lr_scheduler, early_stopping], verbose=0)\n",
    "\n",
    "    # Evaluate model\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    val_accuracy = max(history.history['val_accuracy'])\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'layers': layers,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'l1': l1,\n",
    "        'l2': l2,\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_accuracy\n",
    "    })\n",
    "\n",
    "    print(f\"Layers: {layers}, Dropout: {dropout_rate}, L1: {l1}, L2: {l2}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Find the best configuration\n",
    "best_result = max(results, key=lambda x: x['val_accuracy'])\n",
    "print(f\"\\nBest configuration: {best_result}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = Sequential()\n",
    "best_model.add(Dense(best_result['layers'][0], activation='relu', input_shape=(X_train.shape[1],),\n",
    "                     kernel_initializer=initializers.glorot_uniform(), kernel_regularizer=regularizers.l1_l2(l1=best_result['l1'], l2=best_result['l2'])))\n",
    "best_model.add(BatchNormalization())\n",
    "best_model.add(Dropout(best_result['dropout_rate']))\n",
    "\n",
    "for layer_size in best_result['layers'][1:]:\n",
    "    best_model.add(Dense(layer_size, activation='relu',\n",
    "                         kernel_initializer=initializers.glorot_uniform(), kernel_regularizer=regularizers.l1_l2(l1=best_result['l1'], l2=best_result['l2'])))\n",
    "    best_model.add(BatchNormalization())\n",
    "    best_model.add(Dropout(best_result['dropout_rate']))\n",
    "\n",
    "best_model.add(Dense(len(encoder.classes_), activation='softmax', kernel_initializer=initializers.glorot_uniform()))\n",
    "\n",
    "# Compile the best model\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "best_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the best model\n",
    "best_model.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_val, y_val),\n",
    "                class_weight=class_weights, callbacks=[lr_scheduler, early_stopping], verbose=0)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "# Convert the encoder.classes_ values to string once\n",
    "target_names = [str(class_name) for class_name in encoder.classes_]\n",
    "\n",
    "# Print classification report without duplicates\n",
    "print(classification_report(y_test, y_pred_classes, target_names=target_names))\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Verdadero')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
